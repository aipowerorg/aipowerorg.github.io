"use strict";(self.webpackChunkhelp=self.webpackChunkhelp||[]).push([[7056],{6843:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>x,contentTitle:()=>g,default:()=>v,frontMatter:()=>p,metadata:()=>s,toc:()=>j});const s=JSON.parse('{"id":"Chat/voice-features","title":"Voice Agents","description":"AIP brings your chatbot to life with two distinct voice functionalities: the new Realtime Voice Agent for live, spoken conversations, and the classic Voice Input/Playback for transcribing user speech and reading bot responses aloud.","source":"@site/docs/Chat/voice-features.md","sourceDirName":"Chat","slug":"/voice-features","permalink":"/docs/voice-features","draft":false,"unlisted":false,"editUrl":"https://github.com/aipowerorg/aipowerorg.github.io/edit/main/docs/Chat/voice-features.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"voice-features","slug":"/voice-features","title":"Voice Agents","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Image","permalink":"/docs/image-features"},"next":{"title":"Security & Privacy","permalink":"/docs/security-privacy"}}');var i=t(4848),o=t(8453);const r=t.p+"assets/images/realtime-voice-addon-ad346caad30c56ee55d3bac6761b8c73.png",a=t.p+"assets/images/realtime-voice-settings-5bf323d261e9b37530f6777d0be49295.png",c=t.p+"assets/images/realtime-voice-button-7e7e0a01f07b564966b9486254895027.png",l=t.p+"assets/images/voice-input-enable-76a8a711bbf08a5474a697e3a953535d.png",d=t.p+"assets/images/voice-playback-addon-39b070f89d6febf8cf941ff39b28b094.png",h=t.p+"assets/images/elevenlabs-settings-9af135bc80a244edea04b9015edaef55.png",u=t.p+"assets/images/tts-enable-6aebeebd32168ea1822eba358b1543cc.png",p={id:"voice-features",slug:"/voice-features",title:"Voice Agents",sidebar_position:6},g="Voice Features",x={},j=[{value:"Realtime Voice Agent",id:"realtime-voice-agent",level:2},{value:"How to Enable",id:"how-to-enable",level:3},{value:"Configuration",id:"configuration",level:3},{value:"How It Works",id:"how-it-works",level:3},{value:"Classic Voice Features",id:"classic-voice-features",level:2},{value:"Voice Input (Speech-to-Text)",id:"voice-input-speech-to-text",level:3},{value:"How to Enable",id:"how-to-enable-1",level:4},{value:"Voice Playback (Text-to-Speech)",id:"voice-playback-text-to-speech",level:3},{value:"How to Enable",id:"how-to-enable-2",level:4},{value:"Configuration",id:"configuration-1",level:4}];function b(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-features",children:"Voice Features"})}),"\n",(0,i.jsxs)(n.p,{children:["AIP brings your chatbot to life with two distinct voice functionalities: the new ",(0,i.jsx)(n.strong,{children:"Realtime Voice Agent"})," for live, spoken conversations, and the classic ",(0,i.jsx)(n.strong,{children:"Voice Input/Playback"})," for transcribing user speech and reading bot responses aloud."]}),"\n",(0,i.jsx)(n.h2,{id:"realtime-voice-agent",children:"Realtime Voice Agent"}),"\n",(0,i.jsx)(n.p,{children:"This feature enables low-latency, speech-to-speech conversational experiences using OpenAI's Realtime API. When activated, users can talk to your chatbot and receive spoken replies in near real-time, creating a fluid and natural voice agent."}),"\n",(0,i.jsxs)(n.admonition,{title:"This is a Pro Feature",type:"info",children:[(0,i.jsxs)(n.p,{children:["To use this feature, you need a Pro plan and the ",(0,i.jsx)(n.strong,{children:"Realtime Voice"})," addon must be enabled from the ",(0,i.jsx)(n.strong,{children:"AIP > Add-ons"})," page."]}),(0,i.jsx)("img",{src:r})]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["This feature only works with OpenAI's Realtime models (",(0,i.jsx)(n.code,{children:"gpt-4o-realtime-preview"})," and ",(0,i.jsx)(n.code,{children:"gpt-4o-mini-realtime"}),"). Ensure your OpenAI API key is configured in the main dashboard."]})}),"\n",(0,i.jsx)(n.h3,{id:"how-to-enable",children:"How to Enable"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Navigate to your chatbot's settings in ",(0,i.jsx)(n.strong,{children:"AIP > Chat"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Open the ",(0,i.jsx)(n.strong,{children:"Voice Agent"})," accordion and check the ",(0,i.jsx)(n.strong,{children:"Enable Realtime Voice Agent"})," box."]}),"\n"]}),"\n",(0,i.jsx)("img",{src:a}),"\n",(0,i.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Realtime Model"}),": Choose between ",(0,i.jsx)(n.code,{children:"gpt-4o-realtime-preview"})," or the mini version."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice"}),": Select the OpenAI voice you want the agent to use (e.g., Alloy, Shimmer)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Turn Detection"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"None (Push-to-Talk)"}),": The user must click the button to start and stop speaking."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Server VAD"}),": OpenAI's server detects when the user stops speaking based on silence."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic VAD"}),": A more advanced detection that understands the context of the speech to determine when a turn is complete."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response Speed"}),": Control how quickly the AI speaks, from ",(0,i.jsx)(n.code,{children:"0.25"})," (slowest) to ",(0,i.jsx)(n.code,{children:"1.5"})," (fastest)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced Settings"}),": Configure technical details like audio formats and noise reduction. The defaults are recommended for most use cases."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,i.jsx)(n.p,{children:"When enabled, a new voice button appears in the chat input area."}),"\n",(0,i.jsx)("img",{src:c}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'The user clicks the button to start the session. The button will indicate it\'s "Connecting...".'}),"\n",(0,i.jsx)(n.li,{children:'Once connected, the button changes to a "Listening" state. The user can start speaking.'}),"\n",(0,i.jsxs)(n.li,{children:["Based on your ",(0,i.jsx)(n.strong,{children:"Turn Detection"})," setting, the AI will either wait for the user to click the button again or automatically detect when they've finished speaking."]}),"\n",(0,i.jsx)(n.li,{children:'The AI processes the speech and responds with voice. While the AI is speaking, the button changes to a "Speaking" state.'}),"\n",(0,i.jsx)(n.li,{children:"The conversation continues until the user clicks the button to end the session."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"classic-voice-features",children:"Classic Voice Features"}),"\n",(0,i.jsx)(n.p,{children:"These features add basic voice functionality to the standard text-based chat experience."}),"\n",(0,i.jsx)(n.h3,{id:"voice-input-speech-to-text",children:"Voice Input (Speech-to-Text)"}),"\n",(0,i.jsx)(n.p,{children:"This feature adds a microphone icon to the chat input field, allowing users to speak their messages, which are then transcribed into text and sent."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["This feature uses ",(0,i.jsx)(n.strong,{children:"OpenAI's Whisper model"})," for transcription. Ensure your OpenAI API key is configured in the main dashboard."]})}),"\n",(0,i.jsx)(n.h4,{id:"how-to-enable-1",children:"How to Enable"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Navigate to your chatbot's settings in ",(0,i.jsx)(n.strong,{children:"AIP > Chat"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Open the ",(0,i.jsx)(n.strong,{children:"General"})," accordion."]}),"\n",(0,i.jsx)(n.li,{children:"Click the microphone icon next to the model selector to enable voice input for the chatbot."}),"\n"]}),"\n",(0,i.jsx)("img",{src:l}),"\n",(0,i.jsx)(n.h3,{id:"voice-playback-text-to-speech",children:"Voice Playback (Text-to-Speech)"}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsxs)(n.p,{children:["AIP supports three TTS providers: ",(0,i.jsx)(n.strong,{children:"OpenAI"}),", ",(0,i.jsx)(n.strong,{children:"Google"}),", and ",(0,i.jsx)(n.strong,{children:"ElevenLabs"}),"."]}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"OpenAI"})," and ",(0,i.jsx)(n.strong,{children:"Google"}),", the plugin uses the API keys you entered under ",(0,i.jsx)(n.strong,{children:"Main Dashboard > Providers"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"ElevenLabs"}),", once the ",(0,i.jsx)(n.strong,{children:"Voice Playback (TTS)"})," add-on is activated, go to ",(0,i.jsx)(n.strong,{children:"Main Dashboard > Integrations"})," to enter your ElevenLabs API key and sync available models and voices."]}),"\n"]}),(0,i.jsx)("img",{src:h,width:"600"})]}),"\n",(0,i.jsx)(n.p,{children:'This feature adds a "play" button to each of the chatbot\'s responses, allowing users to listen to the message.'}),"\n",(0,i.jsx)(n.h4,{id:"how-to-enable-2",children:"How to Enable"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to ",(0,i.jsx)(n.strong,{children:"AIP > Add-ons"})," and activate the ",(0,i.jsx)(n.strong,{children:"Voice Playback (TTS)"})," add-on. This is a free add-on."]}),"\n"]}),"\n",(0,i.jsx)("img",{src:d,width:"600"}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["Return to your chatbot's settings in ",(0,i.jsx)(n.strong,{children:"AIP > Chat"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Open the ",(0,i.jsx)(n.strong,{children:"Voice Playback"})," accordion and check the ",(0,i.jsx)(n.strong,{children:"Enable Voice Playback"})," box."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"configuration-1",children:"Configuration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TTS Provider"}),": Choose the service to generate the audio. Options include Google, OpenAI, and ElevenLabs."]}),"\n"]}),"\n",(0,i.jsx)("img",{src:u,width:"600"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice"}),": Select a specific voice.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"Google"})," and ",(0,i.jsx)(n.strong,{children:"ElevenLabs"}),", you can click the ",(0,i.jsx)(n.strong,{children:"Sync"})," button to fetch the voices available in your account."]}),"\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"OpenAI"}),", a list of standard voices is available."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Auto Play"}),": Check this box if you want the chatbot's responses to be read aloud automatically as soon as they are generated."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Auto Play is not compatible with Stream Mode."})," If streaming is enabled, the voice will not play automatically."]})})]})}function v(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(b,{...e})}):b(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);