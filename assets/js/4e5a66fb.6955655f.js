"use strict";(self.webpackChunkhelp=self.webpackChunkhelp||[]).push([[968],{3805:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>b,contentTitle:()=>v,default:()=>I,frontMatter:()=>y,metadata:()=>o,toc:()=>A});const o=JSON.parse('{"id":"ai-providers","title":"AI Providers","description":"AIP > Dashboard is the central area for managing your connections to different AI services and setting the default behavior for all of AIP\'s modules.","source":"@site/docs/ai.md","sourceDirName":".","slug":"/ai-providers","permalink":"/docs/ai-providers","draft":false,"unlisted":false,"editUrl":"https://github.com/aipowerorg/aipowerorg.github.io/edit/main/docs/ai.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"ai-providers","slug":"/ai-providers","title":"AI Providers","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Add-ons","permalink":"/docs/addons"},"next":{"title":"Chat","permalink":"/docs/category/chat"}}');var l=s(4848),i=s(8453);const r=s.p+"assets/images/openai-d8fd39a053f1385c642fcfdf00e52680.png",t=s.p+"assets/images/dashboard-a6830a1fc313f1a4e3f9f3c2cc75649f.png",a=s.p+"assets/images/finetune-712a7d8024f8beb6d64fda4e36100d07.png",d=s.p+"assets/images/openai-settings-c50537d86203f5845a815907e3c553d5.png",c=s.p+"assets/images/google-19e9dafb629c6efc572fc0b85f6a57c3.png",h=s.p+"assets/images/safety-621a04dbe1879086bd147993f5a15396.png",u=s.p+"assets/images/openrouter-4aa089c648bfb51a362ec5977965e2c0.png",p=s.p+"assets/images/azure-0e3f134c6d598fcac426d0b6f939e6b2.png",x=s.p+"assets/images/deepseek-addon-37775ef018e5db7bd06bc0bdc09d4bef.png",g=s.p+"assets/images/deepseek-defa8e267334a4aa8751e29eebfad74e.png",m=s.p+"assets/images/ollama-addon-eb6f72656a1cb877a18ed6a1f234a656.png",j=s.p+"assets/images/ollama-321ba9bb4883ac8f327616d5e2afe994.png",f=s.p+"assets/images/parameters-e1abdd9854a4a374e5f4b3df92183bd7.png",y={id:"ai-providers",slug:"/ai-providers",title:"AI Providers",sidebar_position:4},v="Overview",b={},A=[{value:"OpenAI",id:"openai",level:2},{value:"Google",id:"google",level:2},{value:"Azure",id:"azure",level:2},{value:"OpenRouter",id:"openrouter",level:2},{value:"DeepSeek",id:"deepseek",level:2},{value:"Ollama (Local AI)",id:"ollama-local-ai",level:2},{value:"Step 1: Install Ollama",id:"step-1-install-ollama",level:3},{value:"On macOS",id:"on-macos",level:4},{value:"On Windows",id:"on-windows",level:4},{value:"On Linux",id:"on-linux",level:4},{value:"Step 2: Download an AI Model",id:"step-2-download-an-ai-model",level:3},{value:"Step 3: Configure AIP Plugin",id:"step-3-configure-aip-plugin",level:3},{value:"Global AI Parameters",id:"global-ai-parameters",level:2}];function w(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"overview",children:"Overview"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"AIP > Dashboard"})," is the central area for managing your connections to different AI services and setting the default behavior for all of AIP's modules."]}),"\n",(0,l.jsx)(n.admonition,{title:"AIP uses a Bring Your Own API Key model",type:"info",children:(0,l.jsxs)(n.p,{children:["To use the plugin, you must have a valid API key from your preferred AI provider (e.g., OpenAI, Google, Azure).\n",(0,l.jsx)(n.strong,{children:"Purchasing the plugin does not include any API credits."})]})}),"\n",(0,l.jsx)(n.p,{children:"You must configure at least one provider here for the plugin to work."}),"\n",(0,l.jsx)("img",{src:t}),"\n",(0,l.jsx)(n.h2,{id:"openai",children:"OpenAI"}),"\n",(0,l.jsx)(n.p,{children:"OpenAI provides popular gpt and image generation models."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Configuration Steps"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select the Provider"}),": From the ",(0,l.jsx)(n.strong,{children:"Engine"})," dropdown menu, choose ",(0,l.jsx)(n.strong,{children:"OpenAI"}),". The provider selected here will also be the default for all modules."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enter API Key"}),": In the API Key field, enter your secret key from OpenAI. You can click the ",(0,l.jsx)(n.strong,{children:"Get Key"})," button to go directly to the ",(0,l.jsx)(n.a,{href:"https://platform.openai.com/",children:"OpenAI API Keys page"}),"."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:r,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sync Models"}),": Click the ",(0,l.jsx)(n.strong,{children:"Sync"})," button. AIP will connect to OpenAI and download a list of all AI models available to your account, including any custom fine-tuned models you have."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:a,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select a Default Model"}),": Once the sync is complete, the ",(0,l.jsx)(n.strong,{children:"Model"})," dropdown will be populated. Select a default model for the plugin to use (e.g., ",(0,l.jsx)(n.code,{children:"gpt-4o-mini"}),")."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"OpenAI-Specific Settings"})}),"\n",(0,l.jsxs)(n.p,{children:["These settings are found in the ",(0,l.jsx)(n.strong,{children:"Parameters"})," accordion when OpenAI is the selected provider."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Base URL:"})," This setting allows you to use a proxy service instead of the default ",(0,l.jsx)(n.code,{children:"https://api.openai.com"})," endpoint."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Store Conversation:"})," When enabled, OpenAI will store conversations for 30 days. You can view them ",(0,l.jsx)(n.a,{href:"https://platform.openai.com/logs",children:"OpenAI Logs page"}),". This must be enabled if you wish to use the ",(0,l.jsx)(n.strong,{children:"stateful conversation"})," feature in the Chatbot module."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Vector Store File Expiration:"})," Sets the default number of days (1-365) that files uploaded to an OpenAI Vector Store will be kept before being automatically deleted. The default is 7 days. This setting only applies to files uploaded by users via the frontend Chatbot and AI Forms modules.\nIt's a good idea to keep this value low to ensure temporary uploads are cleaned up regularly, reducing storage load and privacy risks."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:d,width:"600"}),"\n",(0,l.jsx)(n.h2,{id:"google",children:"Google"}),"\n",(0,l.jsx)(n.p,{children:"Google provides the Gemini family of models."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Configuration Steps"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select the Provider"}),": From the ",(0,l.jsx)(n.strong,{children:"Engine"})," dropdown menu, choose ",(0,l.jsx)(n.strong,{children:"Google"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enter API Key"}),": Enter your API key from the Google AI Studio."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:c,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sync Models"}),": Click the ",(0,l.jsx)(n.strong,{children:"Sync"})," button to fetch available Gemini models."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select a Default Model"}),": Choose a default model, such as ",(0,l.jsx)(n.code,{children:"gemini-1.5-pro-latest"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Google-Specific Settings"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Base URL:"})," This setting allows you to use a proxy service instead of the default ",(0,l.jsx)(n.code,{children:"https://generativelanguage.googleapis.com"})," endpoint."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Safety Settings"}),": Found in the ",(0,l.jsx)(n.strong,{children:"Parameters"})," accordion, this allows you to configure Google's content safety filters. You can set a blocking threshold (from Block None to Block Most) for categories like Harassment, Hate Speech, Sexually Explicit, and Dangerous Content."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:h,width:"600"}),"\n",(0,l.jsx)(n.h2,{id:"azure",children:"Azure"}),"\n",(0,l.jsx)(n.p,{children:"Azure allows you to use ai models hosted on the Microsoft Azure platform."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Configuration Steps"})}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select the Provider"}),": From the ",(0,l.jsx)(n.strong,{children:"Engine"})," dropdown menu, choose ",(0,l.jsx)(n.strong,{children:"Azure"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enter API Key and Endpoint"}),": You must provide both your ",(0,l.jsx)(n.strong,{children:"API Key"})," and your unique ",(0,l.jsx)(n.strong,{children:"Endpoint URL"})," from your Azure AI Studio resource."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:p,width:"600"}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sync Deployments"}),': For Azure, the "Sync" button fetches your ',(0,l.jsx)(n.strong,{children:"Deployments"}),", not the base models. You must first deploy a model (e.g., gpt-4o) in Azure to have it appear in the list."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select a Default Deployment"}),": Choose one of your synced deployments as the default."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Azure-Specific Settings"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Endpoint URL:"})," This is the endpoint URL for your specific Azure AI resource. It is a required field."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"openrouter",children:"OpenRouter"}),"\n",(0,l.jsx)(n.p,{children:"OpenRouter is a service that gives you access to a wide variety of models from different companies (like Anthropic's Claude, Mistral, and Meta's Llama) using a single API key."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Configuration Steps"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select the Provider"}),": From the ",(0,l.jsx)(n.strong,{children:"Engine"})," dropdown menu, choose ",(0,l.jsx)(n.strong,{children:"OpenRouter"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enter API Key"}),": Enter your API key from OpenRouter."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:u,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sync Models"}),": Click the ",(0,l.jsx)(n.strong,{children:"Sync"})," button. This will fetch a large list of available models."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select a Default Model"}),": Choose a default model from the list, such as ",(0,l.jsx)(n.code,{children:"anthropic/claude-3.7-sonnet"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"OpenRouter-Specific Settings"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Base URL:"})," This setting allows you to use a proxy service instead of the default ",(0,l.jsx)(n.code,{children:"https://openrouter.ai/api"})," endpoint."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"deepseek",children:"DeepSeek"}),"\n",(0,l.jsx)(n.p,{children:"DeepSeek is an AI provider known for its powerful free chat and coding models."}),"\n",(0,l.jsx)(n.p,{children:"DeepSeek is a free add-on."}),"\n",(0,l.jsx)(n.p,{children:"Unlike other providers, DeepSeek is not available by default. You must enable it first from Add-ons page."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Configuration Steps"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enable the Addon"}),": Go to the ",(0,l.jsx)(n.strong,{children:"AIP > Addons"})," page and enable the ",(0,l.jsx)(n.strong,{children:"DeepSeek"})," addon."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:x,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select the Provider"}),": After enabling, go to the ",(0,l.jsx)(n.strong,{children:"Engine"})," dropdown menu and choose ",(0,l.jsx)(n.strong,{children:"DeepSeek"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enter API Key"}),": Enter your API key from the DeepSeek platform."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:g,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sync Models"}),": Click the ",(0,l.jsx)(n.strong,{children:"Sync"})," button to fetch the available models."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select a Default Model"}),": Choose a default model, such as ",(0,l.jsx)(n.code,{children:"deepseek-chat"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"ollama-local-ai",children:"Ollama (Local AI)"}),"\n",(0,l.jsx)(n.p,{children:"Ollama allows you to run powerful, open-source large language models locally on your own computer. This is a Pro feature that offers maximum privacy and control over your AI operations."}),"\n",(0,l.jsxs)(n.admonition,{title:"This is a Pro Feature",type:"info",children:[(0,l.jsxs)(n.p,{children:["To use Ollama, you need a Pro plan and the ",(0,l.jsx)(n.strong,{children:"Ollama Integration"})," addon must be enabled from the ",(0,l.jsx)(n.strong,{children:"AIP > Add-ons"})," page."]}),(0,l.jsx)("img",{src:m})]}),"\n",(0,l.jsx)(n.h3,{id:"step-1-install-ollama",children:"Step 1: Install Ollama"}),"\n",(0,l.jsx)(n.p,{children:"First, you need to download and run the Ollama application on the computer you want to use as the AI server. This could be your local machine or a dedicated server."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Go to the official download page: ",(0,l.jsx)(n.a,{href:"https://ollama.com/download",children:"ollama.com/download"})]}),"\n",(0,l.jsx)(n.li,{children:"Download the installer for your operating system."}),"\n"]}),"\n",(0,l.jsx)(n.h4,{id:"on-macos",children:"On macOS"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Download the ",(0,l.jsx)(n.code,{children:"Ollama-macOS.zip"})," file."]}),"\n",(0,l.jsxs)(n.li,{children:["Unzip the file and move the ",(0,l.jsx)(n.strong,{children:"Ollama"})," application to your ",(0,l.jsx)(n.code,{children:"Applications"})," folder."]}),"\n",(0,l.jsx)(n.li,{children:"Run the Ollama application. An icon will appear in your menu bar, indicating that the server is running."}),"\n"]}),"\n",(0,l.jsx)(n.h4,{id:"on-windows",children:"On Windows"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Download and run the ",(0,l.jsx)(n.code,{children:"OllamaSetup.exe"})," installer."]}),"\n",(0,l.jsx)(n.li,{children:"Follow the on-screen instructions to complete the installation."}),"\n",(0,l.jsx)(n.li,{children:"Ollama will run automatically in the background. An icon will appear in your system tray."}),"\n"]}),"\n",(0,l.jsx)(n.h4,{id:"on-linux",children:"On Linux"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Open your terminal."}),"\n",(0,l.jsxs)(n.li,{children:["Run the official installation script with the following command:","\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://ollama.com/install.sh | sh\n"})}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["The script will set up Ollama as a ",(0,l.jsx)(n.code,{children:"systemd"})," service, which will start automatically."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"step-2-download-an-ai-model",children:"Step 2: Download an AI Model"}),"\n",(0,l.jsxs)(n.p,{children:['With the Ollama application running, you need to download (or "pull") a model. You can find a list of available models in the ',(0,l.jsx)(n.a,{href:"https://ollama.com/library",children:"Ollama Library"}),"."]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Open your terminal (on macOS/Linux) or Command Prompt (on Windows)."}),"\n",(0,l.jsxs)(n.li,{children:["Run the ",(0,l.jsx)(n.code,{children:"ollama pull"})," command followed by the model name. For example, to download Llama 3:","\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ollama pull llama3\n"})}),"\n","This will download the model to your computer. You can pull as many models as you like."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"step-3-configure-aip-plugin",children:"Step 3: Configure AIP Plugin"}),"\n",(0,l.jsx)(n.p,{children:"Now, connect your WordPress site to your running Ollama instance."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enable the Addon"}),": Go to ",(0,l.jsx)(n.strong,{children:"AIP > Add-ons"})," and activate the ",(0,l.jsx)(n.strong,{children:"Ollama Integration"})," addon."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select the Provider"}),": Go to ",(0,l.jsx)(n.strong,{children:"AIP > Dashboard"}),". From the ",(0,l.jsx)(n.strong,{children:"Engine"})," dropdown, choose ",(0,l.jsx)(n.strong,{children:"Ollama"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enter Base URL"}),": Enter the URL where your Ollama server is running. If you are running it on the same computer as your local WordPress development site, the default URL is ",(0,l.jsx)(n.code,{children:"http://localhost:11434"}),"."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:j}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Sync Models"}),": Click the ",(0,l.jsx)(n.strong,{children:"Sync"})," button. AIP will connect to your Ollama server and fetch the list of models you have pulled."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Select a Default Model"}),": Choose one of your local models as the default."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Your site is now configured to use your local Ollama models!"}),"\n",(0,l.jsx)(n.h2,{id:"global-ai-parameters",children:"Global AI Parameters"}),"\n",(0,l.jsxs)(n.p,{children:["This section, found in the ",(0,l.jsx)(n.strong,{children:"Parameters"})," accordion, controls the default behavior of the AI across all modules. These settings can usually be overridden in specific modules (like the content enhancer tools)."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Max Tokens:"})," Sets the maximum length of the AI's response. A higher number allows for longer content but may increase costs."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Temperature:"})," Controls the creativity of the AI. A higher value (e.g., ",(0,l.jsx)(n.code,{children:"1.2"}),") makes the output more random and creative. A lower value (e.g., ",(0,l.jsx)(n.code,{children:"0.5"}),") makes it more focused and predictable."]}),"\n"]}),"\n",(0,l.jsx)("img",{src:f,width:"600"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Top P:"})," An alternative method to Temperature for controlling randomness. It is recommended to alter only one of these, not both."]}),"\n"]})]})}function I(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(w,{...e})}):w(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>t});var o=s(6540);const l={},i=o.createContext(l);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);